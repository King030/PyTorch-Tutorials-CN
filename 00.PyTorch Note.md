# 01.Tensors

张量是一种特殊的数据结构，与数组和矩阵非常相似。在 PyTorch 中，我们使用张量对模型的输入和输出以及模型的参数进行编码。

张量类似于 NumPy 的 ndarray，只不过张量可以在 GPU 或其他硬件加速器上运行。事实上，张量和 NumPy 数组通常可以共享相同的底层内存，这样就不需要复制数据了(参见使用 NumPy 的 Bridge)。张量也针对自动微分进行了优化(我们将在后面的 Autograd 部分中看到更多)。如果您熟悉 ndarray，那么您就会熟悉 Tensor API。如果没有，跟着来！

~~~
import torch
import numpy as np
~~~



## Initializing a Tensor

从数据中直接创建

张量可以直接从数据中创建。数据类型会自动推断。

```
data = [[1, 2],[3, 4]]
x_data = torch.tensor(data)
```

从NumPy数组中创建

可以从NumPy数组创建张量（反之亦然-请参阅与NumPy的结合）。

```
np_array = np.array(data)
x_np = torch.from_numpy(np_array)
```

从另一个张量中创建：

新的张量保留参数张量的属性（形状，数据类型），除非明确覆盖。

```
x_ones = torch.ones_like(x_data) #保留x_data的属性
print(f"Ones Tensor: \n {x_ones} \n")

x_rand = torch.rand_like(x_data, dtype=torch.float) #重写x_data的属性
print(f"Random Tensor: \n {x_rand} \n")
```

**What is shape?**

shape是一个张量维度的元组。在下面的函数中确定了输出张量的维数

~~~
shape = (2,3,)
rand_tensor = torch.rand(shape)
ones_tensor = torch.ones(shape)
zeros_tensor = torch.zeros(shape)

print(f"Random Tensor: \n {rand_tensor} \n")
print(f"Ones Tensor: \n {ones_tensor} \n")
print(f"Zeros Tensor: \n {zeros_tensor}")
~~~

![image-20230515220300759](../../../AppData/Roaming/Typora/typora-user-images/image-20230515220300759.png)

## Attributes of a Tensor

Tensor attributes describe their shape, datatype, and the device on which they are stored.

~~~
tensor = torch.rand(3,4)

print(f"Shape of tensor: {tensor.shape}")
print(f"Datatype of tensor: {tensor.dtype}")
print(f"Device tensor is stored on: {tensor.device}")
~~~

![image-20230515220312819](../../../AppData/Roaming/Typora/typora-user-images/image-20230515220312819.png)

## Operations on Tensors

张量操作中包括100多种操作，包括算术运算、线性代数、矩阵操作（转置、索引、切片）、采样等等including arithmetic, linear algebra, matrix manipulation (transposing, indexing, slicing)。这里全面介绍了这些操作。

每个操作都可以在GPU上运行（通常比在CPU上运行速度更快）。如果您正在使用Colab，可以通过转到“运行时”>“更改运行时类型”>“GPU”来分配一个GPU。

默认情况下，张量是在CPU上创建的。我们需要使用.to方法显式地将张量移动到GPU上（在检查GPU是否可用后）。请记住，在设备之间复制大型张量可能会消耗大量时间和内存！

~~~
# We move our tensor to the GPU if available
if torch.cuda.is_available():
    tensor = tensor.to("cuda")
~~~

尝试使用列表中的一些操作。如果你熟悉NumPy API，你会发现Tensor API非常容易使用。

标准的类似于NumPy的索引和切片操作：

~~~
tensor = torch.ones(4, 4)
print(f"First row: {tensor[0]}")
print(f"First column: {tensor[:, 0]}")
print(f"Last column: {tensor[..., -1]}")
tensor[:,1] = 0
print(tensor)
~~~

![image-20230515220542620](../../../AppData/Roaming/Typora/typora-user-images/image-20230515220542620.png)

torch.cat是一个PyTorch中的张量拼接函数，它可以沿着给定的维度连接一系列张量。与之类似的函数还包括torch.stack，它也可以用于张量的拼接，但是与torch.cat有一些微妙的区别。

![image-20230514222949650](C:\Users\10168\AppData\Roaming\Typora\typora-user-images\image-20230514222949650.png)

**Arithmetic operations**

1. `y1 = tensor @ tensor.T`：计算`tensor`与其转置的矩阵乘积，得到一个新的张量`y1`。
2. `y2 = tensor.matmul(tensor.T)`：计算`tensor`与其转置的矩阵乘积，得到一个新的张量`y2`。这里使用了`matmul`函数，它与`@`符号的作用相同。
3. `y3 = torch.rand_like(tensor)`：生成一个与`tensor`形状相同的随机张量`y3`，其中的每个元素都是从均匀分布中随机采样得到的。

4.`torch.matmul(tensor, tensor.T, out=y3)`

这行代码使用了PyTorch中的`matmul`函数，它用于计算两个张量的矩阵乘积。具体来说，`torch.matmul(tensor, tensor.T)`将计算`tensor`与其转置的矩阵乘积，并返回一个新的张量。此外，这里还指定了`out`参数，它表示输出结果要存储在预先分配的张量`y3`中。

1. `z1 = tensor * tensor` 和 `z2 = tensor.mul(tensor)`：这两个操作都用于计算两个张量的逐元素积。`*`符号和`mul`函数都表示逐元素积的操作，结果是一个新的张量。这里`z1`和`z2`分别表示`tensor`与自身的逐元素积。

2. `torch.mul(tensor, tensor, out=z3)`：这个操作与上面的逐元素积操作类似，用于计算两个张量的逐元素积，并将结果存储在预先分配的张量`z3`中。`torch.mul`函数和`*`符号的作用相同。需要注意的是，如果`z3`的形状不匹配或不合适，则会抛出异常。

因此，这段代码的含义是，分别计算`tensor`与自身的逐元素积，并将结果存储在`z1`和`z2`中。另外，使用`torch.mul`函数将`tensor`与自身的逐元素积计算出来，并将结果存储在预先分配的张量`z3`中。

**Single-element tensors**
在PyTorch中，当一个张量只有一个元素时，我们称之为单元素张量（single-element tensor）。如果想将单元素张量转换为Python数值类型，可以使用`.item()`函数。

~~~
agg = tensor.sum()
agg_item = agg.item()
print(agg_item, type(agg_item))
~~~

例如，如果`t`是一个单元素张量，我们可以使用`t.item()`将其转换为Python数值类型。

`tensor.sum()`是一个PyTorch张量的方法，用于计算张量中所有元素的总和。数量和

**In-place operations**  JUST IGNORE IT THE USE OF IT IS DISCOURAGED

In-place操作指的是将操作结果直接存储到操作数中的操作。这些操作通常在函数名后面加上`_`后缀，例如`x.copy_(y)`和`x.t_()`。执行这些操作后，变量`x`的值将被修改为操作的结果。

## Bridge with NumPy

Tensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other.

# 02.DATASETS & DATALOADERS

处理数据样本的代码可能变得混乱且难以维护；理想情况下，我们希望我们的数据集代码与模型训练代码解耦，以获得更好的可读性和模块化。

PyTorch提供了两个数据基元：torch.utils.data.DataLoader和torch.utils.data.Dataset，允许您使用预加载的数据集以及自己的数据。数据集存储样本及其相应的标签，DataLoader包装了一个可迭代对象，使得轻松访问样本变得容易。

**torch.utils.data.DataLoader**是PyTorch提供的一个用于加载数据的工具，可以将数据集包装成一个可迭代对象，使得可以方便地进行批处理、多线程数据预处理等操作。通过DataLoader可以指定批次大小、数据加载方式、是否打乱数据等参数，实现高效的数据加载。同时，DataLoader也可以配合使用数据集类（如torch.utils.data.Dataset）来自定义数据集，并进行扩展。

**torch.utils.data.Dataset**是PyTorch提供的一个抽象类，用于表示数据集。通过继承该类并实现其中的抽象方法，可以自定义各种数据集并进行扩展。在使用PyTorch进行深度学习任务时，通常需要先将数据集转化为Dataset对象，并使用DataLoader对其进行批处理、多线程数据预处理等操作，实现高效的数据加载。Dataset对象通常需要实现__len__和__getitem__两个方法，分别用于返回数据集的大小和指定索引处的数据样本，使得Dataset对象可以被迭代器进行遍历。

PyTorch领域库提供了许多预加载的数据集（如FashionMNIST），它们是torch.utils.data.Dataset的子类，并实现特定于特定数据的函数。它们可以用于原型设计和模型基准测试。您可以在这里找到它们：图像数据集，文本数据集和音频数据集。

## Loading a Dataset

以下是如何从TorchVision加载Fashion-MNIST数据集的示例。Fashion-MNIST是由Zalando的文章图像组成的数据集，包括60,000个训练示例和10,000个测试示例。每个示例包括一个28×28的灰度图像和与之相关联的来自10个类别中的一个标签。

我们使用以下参数加载FashionMNIST数据集: 

root是训练/测试数据存储的路径，

train指定训练或测试数据集，

download=True如果在root中不可用（root中找不到所需数据），则从互联网下载数据。

transform和target_transform指定特征和标签转换。

~~~
import torch
from torch.utils.data import Dataset
from torchvision import datasets
from torchvision.transforms import ToTensor
import matplotlib.pyplot as plt


training_data = datasets.FashionMNIST(
    root="data",
    train=True,
    download=True,
    transform=ToTensor()
)

test_data = datasets.FashionMNIST(
    root="data",
    train=False,
    download=True,
    transform=ToTensor()
)
~~~



## Iterating and Visualizing the Dataset

迭代和可视化数据集

我们可以像列表一样手动索引数据集：training_data[index]。我们使用matplotlib可视化我们训练数据中的一些样本。

~~~
labels_map = {
    0: "T-Shirt",
    1: "Trouser",
    2: "Pullover",
    3: "Dress",
    4: "Coat",
    5: "Sandal",
    6: "Shirt",
    7: "Sneaker",
    8: "Bag",
    9: "Ankle Boot",
}
figure = plt.figure(figsize=(8, 8))
cols, rows = 3, 3
for i in range(1, cols * rows + 1):
    sample_idx = torch.randint(len(training_data), size=(1,)).item()
    img, label = training_data[sample_idx]
    figure.add_subplot(rows, cols, i)
    plt.title(labels_map[label])
    plt.axis("off")
    plt.imshow(img.squeeze(), cmap="gray")
plt.show()
~~~



## Creating a Custom Dataset for your files

为文件创建自定义数据集

自定义的 Dataset 类必须实现三个函数：\_\_init\__, \_\_len\_\_, and \_\_getitem\_\_. 看一下这个实现；FashionMNIST 图像存储在一个名为`img_dir`的目录中，它们的标签单独存储在 CSV 文件 `annotations_file `中。

这些是Python中用于自定义类的特殊方法（也称为“魔术方法”）。

- `__init__`是一个类的构造函数，用于在创建对象时初始化其属性。
- `__len__`是一个类的方法，用于返回对象的长度。
- `__getitem__`是一个类的方法，用于通过索引或键获取对象的元素。

这些方法的实现允许类的实例像内置类型（例如列表、字典等）一样进行操作，从而使开发人员能够创建更加灵活和功能强大的自定义数据类型。



在接下来的几节中，我们将分解每个函数中正在发生的事情。

~~~
import os
import pandas as pd
from torchvision.io import read_image

class CustomImageDataset(Dataset):
    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):
        self.img_labels = pd.read_csv(annotations_file)
        self.img_dir = img_dir
        self.transform = transform
        self.target_transform = target_transform

    def __len__(self):
        return len(self.img_labels)

    def __getitem__(self, idx):
        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])
        image = read_image(img_path)
        label = self.img_labels.iloc[idx, 1]
        if self.transform:
            image = self.transform(image)
        if self.target_transform:
            label = self.target_transform(label)
        return image, label
~~~



### \_\_init\__

\_\_init\__函数是在实例化Dataset对象时运行一次的函数。我们初始化包含图像的目录、注释文件( annotations file)和两个转换函数（在下一节中详细介绍）。

labels.csv文件看起来像：

![image-20230515001547545](../../../AppData/Roaming/Typora/typora-user-images/image-20230515001547545.png)

~~~
def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):
    self.img_labels = pd.read_csv(annotations_file)
    self.img_dir = img_dir
    self.transform = transform
    self.target_transform = target_transform
~~~

这段代码定义了一个类，名为 `Dataset`，它是一个 PyTorch 中的自定义数据集类。这个类的构造函数 `__init__` 有四个参数：`annotations_file`，`img_dir`，`transform`，`target_transform`。

在 `__init__` 中，首先使用 Pandas 库的 `read_csv` 函数读取了一个叫做 `annotations_file` 的 CSV 文件，并将其保存到了一个名为 `img_labels` 的变量中。这个 CSV 文件应该包含每张图片的文件名和标签信息。

接下来，代码将图像文件的文件夹路径保存到了 `img_dir` 变量中，并将两个图像变换（transform）函数，即 `transform` 和 `target_transform`，分别保存到了类变量 `self.transform` 和 `self.target_transform` 中。

这个自定义数据集类的目的是为了让 PyTorch 的 DataLoader 能够读取这个数据集，并将其用于训练或测试神经网络模型。`__init__` 方法的主要作用是初始化数据集所需的参数和变量。

### \_\_len\_\_

The` __len__` function returns the number of samples in our dataset.

这个函数返回你的数据集中的样例的个数。

![image-20230515002011110](../../../AppData/Roaming/Typora/typora-user-images/image-20230515002011110.png)

### \_\_getitem\_\_

\_\_getitem\_\_函数是 Dataset 类的一个方法，用于获取数据集中特定索引位置的样本。具体来说，它根据索引 `idx` 找到相应图片在磁盘上的位置，读取该图片并用 `read_image` 转换为张量格式。然后，它从 `self.img_labels` 中的csv数据获取该图片对应的标签，并根据需求调用 `transform` 和 `target_transform` 函数来对图片和标签进行进一步的预处理。最后，它将预处理后的图片和标签作为元组返回。

~~~
def __getitem__(self, idx):
    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])
    image = read_image(img_path)
    label = self.img_labels.iloc[idx, 1]
    if self.transform:
        image = self.transform(image)
    if self.target_transform:
        label = self.target_transform(label)
    return image, label
~~~



## Preparing your data for training with DataLoaders

数据集Dataset按一次一个样本的方式检索数据集的特征和标签。在训练模型时，我们通常希望以“小批量”「minibatches」方式传递样本，每个时代重新洗牌数据以减少模型过度拟合，并使用Python的多处理`multiprocessing`来加快数据检索。

DataLoader是一个可迭代对象，可以在简单的API中抽象出这种复杂性。

~~~
from torch.utils.data import DataLoader

train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)
test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)
~~~

这段代码定义了两个DataLoader对象，一个用于训练数据集`train_dataloader`，另一个用于测试数据集`test_dataloader`。其中使用了torch.utils.data中的DataLoader类，它抽象了数据集的复杂性并提供了易于使用的API。在定义DataLoader对象时，需要指定使用的数据集(training_data或test_data)，每个batch的大小(batch_size)和是否对数据进行随机化打乱(shuffle)（中文叫洗牌这个单词）。在训练过程中，通常会设置shuffle=True以在每个epoch重新随机打乱数据，以减少模型的过拟合。

## Iterate through the DataLoader

我们已经将数据集加载到DataLoader中，并可以根据需要迭代(Iterate)数据集。下面的每次迭代都返回一个train_features和train_labels的批次（分别包含batch_size=64个特征和标签）。因为我们指定了shuffle=True，在我们迭代所有批次之后，数据会被重新洗牌（如果需要更细粒度地控制数据加载顺序，请查看Samplers）。

~~~
# Display image and label.
train_features, train_labels = next(iter(train_dataloader))
print(f"Feature batch shape: {train_features.size()}")
print(f"Labels batch shape: {train_labels.size()}")
img = train_features[0].squeeze()
label = train_labels[0]
plt.imshow(img, cmap="gray")
plt.show()
print(f"Label: {label}")
~~~

这段代码展示了如何使用`iter()`和`next()`函数迭代一个`DataLoader`对象，以获取一个batch的数据。该batch中包含64个数据样本。在本例中，我们展示了第一个数据样本的特征和标签，并使用`matplotlib`库将该样本的特征（图像）可视化。`squeeze()`函数可以用来移除图像张量中的所有维度大小为1的维度，从而得到一个2D图像。

# 03.TRANSFORMS

数据并不总是以训练机器学习算法所需的最终处理形式出现。我们使用变换（transforms）来对数据进行一些操作，使其适合训练。

所有 TorchVision 数据集都有两个参数` -transform` 用于修改特征和 `target_transform` 用于修改标签 - 它们接受包含转换逻辑的可调用对象。`torchvision.transforms` 模块提供了一些常用的转换。

FashionMNIST 中的特征是 PIL 图像格式，标签是整数。为了训练，我们需要将特征转换为标准化的张量，将标签转换为 one-hot 编码张量。为了进行这些转换，我们使用 ToTensor 和 Lambda。

~~~
import torch
from torchvision import datasets
from torchvision.transforms import ToTensor, Lambda

ds = datasets.FashionMNIST(
    root="data",
    train=True,
    download=True,
    transform=ToTensor(),
    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))
)
~~~

这段代码使用了TorchVision中的FashionMNIST数据集，将数据集下载到本地并将其转换为PyTorch中的Tensor格式。具体地，使用了ToTensor()将PIL格式的图像转换为PyTorch中的Tensor格式，并使用Lambda将标签转换为one-hot编码格式。其中，Lambda函数是一个将标签y转换为one-hot编码格式的函数，使用了scatter_()函数将标签y的位置上赋值为1，其余位置上赋值为0。

## ToTensor()


ToTensor是一个数据变换(transform)函数，它将输入的**PIL image**或者NumPy ndarray数据类型转化成一个包含像素值的浮点数张量(FloatTensor)。

在进行转换的同时，ToTensor还将输入的图像像素强度值进行了缩放，将像素值从0-255的范围缩放到了0-1的范围内，这个操作保证了数据集在传递到神经网络之前像素的值范围一致，从而加速了训练过程的收敛。

**PIL (Python Imaging Library)**是Python语言中的一个图像处理库，它提供了创建、打开、操作各种格式的图像以及图像处理和图像展示等功能。PIL image就是PIL库中表示图像的一种数据类型，它可以是从文件中读取的图像，也可以是通过代码创建的图像。PIL image对象通常是二维的，表示的是图像的像素点。

**NumPy ndarray**指的是NumPy库中的n维数组对象，其中n表示数组的维度。它是NumPy的核心数据结构，用于在Python中处理和操作多维数组。ndarray数组支持广播操作，可以对整个数组进行数学运算，而无需编写循环。NumPy的ndarray还支持向量化操作和许多线性代数函数，使得在Python中进行科学计算变得更加高效。

## Lambda Transforms

Lambda transforms 是一种可以应用**用户定义**的 lambda 函数的变换。在这里，我们定义了一个函数，将整数转换为 one-hot 编码的张量。它首先创建一个大小为 10 的零张量（数据集中标签的数量 一共十个标签吗不是 十个类），然后调用 scatter_ 函数，该函数在标签 y 给出的索引处分配 value=1 的值。

![image-20230515005945893](../../../AppData/Roaming/Typora/typora-user-images/image-20230515005945893.png)

这段代码是对 `target_transform` 进行设置，用来转换标签数据的格式。其中，`Lambda` 表示一个自定义的转换函数，这里将输入的标签 `y` 转换成了一个 one-hot 编码的 Tensor。

具体来说，首先创建了一个全为零的 Tensor，大小为 10（因为 FashionMNIST 数据集中共有 10 个类别），数据类型为 `torch.float`，然后调用 `scatter_` 函数，在指定的位置（由 `y` 给定）设置值为 1，从而得到一个 one-hot 编码的 Tensor。最终的转换结果就是一个维度为 10 的 Tensor，其中除了 `y` 索引位置上的值为 1，其他位置都为 0。

**One-hot编码**是一种常用的将离散特征转换为连续特征的方法，通常用于机器学习中对**分类变量**进行编码。在一组离散特征中，每个可能的取值被编码为唯一的数字，并且每个数字与一个长度等于可能的取值数的向量相关联。该向量的所有元素都是0，除了该特征的取值所对应的元素，该元素设置为1。这样，每个离散特征都可以表示为一个具有固定长度的二进制向量，可用于训练机器学习模型。

`scatter_` 是 PyTorch 的一个张量操作函数，可以按照指定维度和索引，在张量中进行 in-place 的填充操作。具体来说，它可以在指定的索引位置处，将指定的值按照指定的维度进行填充。

例如，对于一个形状为 `(3, 4)` 的张量 `t`，我们可以通过以下代码将其第 1 列的值设置为 1：

```
t[:, 1].scatter_(dim=0, index=torch.tensor([0, 1, 2]), value=1)
```

在这里，`dim=0` 表示按行操作，`index=torch.tensor([0, 1, 2])` 指定了要进行填充的行索引，`value=1` 指定了要填充的值。

# 04.BUILD THE NEURAL NETWORK

**构建神经网络**

神经网络由在数据上执行操作的层/模块组成。torch.nn命名空间提供了构建自己的神经网络所需的所有构建块。PyTorch中的每个模块都是nn.Module的子类。神经网络本身是一个模块，它由其他模块（层）组成。这种嵌套结构使得构建和管理复杂的架构变得容易。

在接下来的章节中，我们将构建一个神经网络，用于对FashionMNIST数据集中的图像进行分类。

~~~
import os
import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
~~~

## Get Device for Training

我们希望能够在可用的硬件加速器（如GPU或MPS）上训练我们的模型。让我们检查一下是否存在torch.cuda或torch.backends.mps，否则我们将使用CPU。

~~~
device = (
    "cuda"
    if torch.cuda.is_available()
    else "mps"
    if torch.backends.mps.is_available()
    else "cpu"
)
print(f"Using {device} device")
~~~

这段代码的作用是选择适当的设备（硬件加速器）来进行模型的训练。它使用了条件语句和`torch.cuda.is_available()`以及`torch.backends.mps.is_available()`函数来检查是否存在可用的GPU或MPS（MPS是一种多进程服务）。如果存在可用的GPU，则将设备设置为"cuda"；如果存在可用的MPS，则将设备设置为"mps"；否则将设备设置为"cpu"，即使用CPU进行训练。

最后，代码通过打印输出来显示所选设备，输出的格式为"Using {device} device"。这样可以方便地查看所选择的设备是什么。

## Define the Class

我们通过继承nn.Module来定义神经网络，并在\_\_init\_\_方法中初始化神经网络的层。每个nn.Module子类在forward方法中实现对输入数据的操作。

~~~
class NeuralNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, 10),
        )

    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits
~~~

这段代码定义了一个名为`NeuralNetwork`的神经网络类，它是`nn.Module`的子类。在类的初始化方法`__init__`中，我们定义了神经网络的各个层。

- `self.flatten`是一个`nn.Flatten`层，用于将输入的二维图像展平为一维向量。
- `self.linear_relu_stack`是一个`nn.Sequential`容器，它按照顺序包含了几个线性层（`nn.Linear`）和激活函数层（`nn.ReLU`）。这个序列模型由三个线性层组成，分别是输入层维度为28*28(784)（即图像展平后的长度）到512维的线性层，512维到512维的线性层，以及512维到10维(10个类型)的线性层。

在`forward`方法中，我们定义了数据在神经网络中的前向传播过程。首先，输入数据经过`self.flatten`层进行展平。然后，展平后的数据通过`self.linear_relu_stack`序列模型进行处理，得到最终的输出`logits`。

通过定义`forward`方法，我们可以在实例化神经网络对象后调用它来执行数据的前向传播。在训练或推理过程中，输入数据将通过神经网络的层进行计算，并输出相应的结果。

我们创建一个`NeuralNetwork`的实例，并将其移动到指定的设备上，并打印其结构。

~~~
model = NeuralNetwork().to(device)
print(model)
~~~

在这段代码中，我们首先创建了一个`NeuralNetwork`的实例，命名为`model`。然后，使用`to()`方法将模型移动到之前选择的设备上，即`device`。这样，模型将使用指定的设备进行计算和训练。

最后，我们通过打印`model`来显示其结构。`print(model)`将打印出模型的结构信息，包括各个层及其参数的数量和形状。这对于了解模型的结构和参数非常有帮助。


要使用模型，我们将输入数据传递给模型。这将执行模型的前向传播，以及一些后台操作。请不要直接调用`model.forward()`方法！

调用模型并传递输入数据将返回一个二维张量，其中dim=0对应于每个类别的10个原始预测值的输出，而dim=1对应于每个输出值的单独值。我们可以通过将输出张量通过`nn.Softmax`模块的实例来获得预测概率。

~~~
X = torch.rand(1, 28, 28, device=device)
logits = model(X)
pred_probab = nn.Softmax(dim=1)(logits)
y_pred = pred_probab.argmax(1)
print(f"Predicted class: {y_pred}")
~~~

这段代码展示了如何使用模型进行预测。

首先，我们创建了一个形状为`(1, 28, 28)`的随机张量`X`，并将其移动到指定的设备上。

然后，我们将`X`传递给模型`model`，并获取模型的输出张量`logits`，其中包含了每个类别的原始预测值。

接下来，我们通过`nn.Softmax(dim=1)`对`logits`进行softmax操作，以获取预测的概率值`pred_probab`。

然后，我们使用`.argmax(1)`方法从概率值张量`pred_probab`中获取最大值所对应的类别索引，即预测的类别。

最后，我们通过打印输出来显示预测的类别索引，使用`f-string`格式化输出。

注意，由于我们只传递了一个样本作为输入，因此张量的形状为`(1,)`，表示只有一个预测结果。如果要对多个样本进行预测，需要将相应的样本张量进行拼接或批处理操作。

通过这段代码，我们可以获取模型对输入数据的预测类别，并输出预测结果。

## Model Layers

让我们分解一下FashionMNIST模型中的各个层。为了说明这一点，我们将采用一个包含3张大小为28x28的图像的小批量样本，并观察当我们将其传递给神经网络时会发生什么。

~~~
input_image = torch.rand(3,28,28)
print(input_image.size())
~~~

在这里，我们使用`torch.rand()`函数创建了一个大小为`(3, 28, 28)`的随机张量`input_image`。这表示我们有一个包含3张28x28大小的图像的批量。

接下来，我们使用`size()`方法打印输出了`input_image`的尺寸。这将打印出一个元组，其中包含了`input_image`张量的各个维度的大小，即`(3, 28, 28)`。

这样，我们可以看到`input_image`张量的维度情况，即它是一个大小为3x28x28的图像批量。

### nn.Flatten

**展平层**

我们初始化`nn.Flatten`层，将每个2D的28x28图像转换为连续的784个像素值的数组（保持小批量维度在dim=0上）。展平操作

~~~
flatten = nn.Flatten()
flat_image = flatten(input_image)
print(flat_image.size())
~~~

在这段代码中，我们创建了一个`nn.Flatten`的实例，并将其命名为`flatten`。然后，我们将`input_image`张量传递给`flatten`层，即`flatten(input_image)`。这将对输入图像进行展平操作，将每个2D的28x28图像转换为一个连续的1D数组，同时保持小批量维度不变。

最后，我们通过打印`flatten_image`的尺寸来查看展平后的图像数组的大小。这将输出一个元组，表示展平后的图像数组的维度，即`(3, 784)`。其中3是小批量样本的数量，784是每个图像的像素值个数。

这样，我们可以观察到通过`nn.Flatten`层后，输入图像被转换为了一个连续的像素值数组，方便后续的神经网络处理。

### nn.Linear

**线性层**（`nn.Linear`）是一个模块，它使用存储的权重和偏置「weights and biases」对输入进行线性变换。

~~~
layer1 = nn.Linear(in_features=28*28, out_features=20)
hidden1 = layer1(flat_image)
print(hidden1.size())
~~~

在这段代码中，我们创建了一个线性层实例`layer1`，其中`in_features=28*28`表示输入特征的数量为784，`out_features=20`表示输出特征的数量为20。

然后，我们将展平后的图像数组`flat_image`传递给`layer1`，即`hidden1 = layer1(flat_image)`。这将对输入进行线性变换，使用层内存储的权重和偏置对输入进行线性组合。

最后，我们使用`print(hidden1.size())`打印`hidden1`张量的尺寸。这将输出一个元组，表示经过线性变换后的张量的大小。对于这个例子，输出的尺寸将是`(3, 20)`，其中3是输入样本的数量，20是线性层的输出特征数量。

通过这段代码，我们可以观察到经过线性层后，输入数据的维度发生了变化，并得到了具有20个特征的隐藏层输出。

### nn.ReLU

非线性**激活函数**是创建模型输入和输出之间复杂映射的关键。它们在线性变换之后应用，引入非线性，帮助神经网络学习各种现象。

在这个模型中，我们在线性层之间使用了`nn.ReLU`激活函数，但还有其他的激活函数可以用于引入非线性。

~~~
print(f"Before ReLU: {hidden1}\n\n")
hidden1 = nn.ReLU()(hidden1)
print(f"After ReLU: {hidden1}")
~~~

在这段代码中，我们打印了ReLU激活函数应用之前的`hidden1`张量，使用`print(f"Before ReLU: {hidden1}\n\n")`进行输出。

然后，我们通过`nn.ReLU()`对`hidden1`进行ReLU激活函数的应用，即`hidden1 = nn.ReLU()(hidden1)`。

最后，我们再次打印了ReLU激活函数应用之后的`hidden1`张量，使用`print(f"After ReLU: {hidden1}")`进行输出。

通过这段代码，我们可以观察到ReLU激活函数的作用，它将`hidden1`张量中所有负值置零，并保持正值不变。这样可以引入非线性，为模型增加更强的表达能力，以学习更复杂的数据模式。

### nn.Sequential

`nn.Sequential`是一个有序的模块容器，数据按照定义的顺序依次通过所有的模块。你可以使用**s**来快速组合一个网络，比如`seq_modules`。

作用是封装上面的操作 这样很简洁很节省时间

~~~
seq_modules = nn.Sequential(
    flatten,
    layer1,
    nn.ReLU(),
    nn.Linear(20, 10)
)
input_image = torch.rand(3,28,28)
logits = seq_modules(input_image)
~~~

在这段代码中，我们定义了一个顺序容器`seq_modules`，其中包含了多个模块按顺序排列。这个顺序容器中的模块包括：

- `flatten`：展平层，用于将输入的图像展平为一维数组。
- `layer1`：线性层1，对展平后的输入进行线性变换。
- `nn.ReLU()`：ReLU激活函数，引入非线性。
- `nn.Linear(20, 10)`：线性层2，对ReLU激活后的输入进行线性变换，将其映射到10个类别的预测值空间。

然后，我们创建一个大小为`(3, 28, 28)`的随机输入图像张量`input_image`。

接下来，我们将`input_image`传递给`seq_modules`，即`logits = seq_modules(input_image)`。这将按照定义的顺序依次将输入图像通过每个模块，得到最终的输出张量`logits`。

通过这段代码，我们可以看到顺序容器的使用方式，数据会按照模块的顺序通过每个模块进行处理，最终得到模型的输出张量。

### nn.Softmax

神经网络的最后一个线性层返回的是logits，即位于[-无穷, 无穷]范围内的原始值。这些logits将被传递给`nn.Softmax`模块。`nn.Softmax`模块将logits缩放到[0, 1]范围内的值，表示模型对每个类别的预测概率。`dim`参数指示在哪个维度上的值必须求和为1。

~~~
softmax = nn.Softmax(dim=1)
pred_probab = softmax(logits)
~~~

在这段代码中，我们创建了一个`nn.Softmax`实例并命名为`softmax`，同时指定了`dim=1`。这表示在维度1上对值进行求和，使得每个样本的预测概率和为1。

然后，我们将logits张量传递给`softmax`，即`pred_probab = softmax(logits)`。`softmax`模块将对logits进行缩放和归一化操作，将其转换为表示每个类别预测概率的概率分布。

通过这段代码，我们可以观察到`nn.Softmax`模块的使用方式，将logits转换为概率分布，以表示模型对每个类别的预测概率。

## Model Parameters

**模型参数**

神经网络中的许多层都是参数化的，即具有相关联的权重和偏置，在训练过程中进行优化。通过继承`nn.Module`，可以自动跟踪模型对象中定义的所有字段，并可以使用模型的`parameters()`或`named_parameters()`方法访问所有参数。

在这个例子中，我们遍历每个参数，并打印其尺寸和部分数值的预览。

~~~
print(f"Model structure: {model}\n\n")

for name, param in model.named_parameters():
    print(f"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \n")
~~~

在这段代码中，我们首先使用`print(f"Model structure: {model}\n\n")`打印出模型的结构。这将输出模型的字符串表示，显示模型的层次结构和连接方式。

然后，我们通过迭代`model.named_parameters()`获取模型的每个参数。对于每个参数，我们使用`name`变量获取参数的名称，使用`param.size()`获取参数的尺寸，使用`param[:2]`获取参数的前两个值的预览。

通过这段代码，我们可以观察到模型参数的访问方式，以及打印参数的尺寸和部分数值的预览，帮助我们了解模型的结构和参数情况。

# 05.AUTOMATIC DIFFERENTIATION WITH `TORCH.AUTOGRAD`

**使用torch.autograd进行自动微分**

 在训练神经网络时，最常用的算法是反向传播（backpropagation）。在这个算法中，根据损失函数相对于给定参数的梯度来调整参数（模型权重）。

为了计算这些梯度，PyTorch提供了一个内置的微分引擎，称为torch.autograd。它支持对任何计算图进行梯度的自动计算。

考虑一个最简单的单层神经网络，其中包含输入x，参数w和b，以及某个损失函数。可以使用以下方式在PyTorch中定义它：

~~~
import torch

x = torch.ones(5)  # input tensor
y = torch.zeros(3)  # expected output
w = torch.randn(5, 3, requires_grad=True)
b = torch.randn(3, requires_grad=True)
z = torch.matmul(x, w)+b
loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)
~~~

在上述代码中，我们首先定义了输入张量x和期望输出y。然后，我们创建了参数w和b，并将它们的`requires_grad`属性设置为True，以指示PyTorch需要计算它们的梯度。

接下来，我们使用参数w和b对输入x进行线性变换，并将结果存储在变量z中。然后，我们使用torch.nn.functional模块中的`binary_cross_entropy_with_logits`函数计算z和y之间的二元交叉熵损失。

通过使用torch.autograd，PyTorch会自动跟踪所有涉及`requires_grad=True`的张量的计算操作，并构建计算图。当我们计算损失时，PyTorch会根据计算图自动计算梯度，并将梯度存储在相应的参数的`.grad`属性中。

使用自动微分引擎torch.autograd，可以方便地计算神经网络参数的梯度，从而进行反向传播和参数更新。

## Tensors, Functions and Computational graph

在PyTorch中，计算图是由张量（Tensors）和函数（Functions）组成的。张量是多维数组，表示计算图中的节点，而函数则表示计算图中的边。

当我们使用PyTorch进行计算时，实际上是在构建一个计算图。这个计算图记录了我们执行的操作和操作之间的依赖关系。

在计算图中，张量是输入和输出，而函数是执行计算的操作。每个函数都有一个正向计算（forward computation）的过程，它接收输入张量并产生输出张量。此外，函数还会自动跟踪与其相关的梯度信息，以支持自动微分。

当我们对计算图中的某个张量调用`.backward()`方法时，PyTorch会根据该张量与计算图中的其他张量之间的依赖关系，自动计算梯度。这个过程称为自动微分（automatic differentiation）或反向传播（backpropagation）。

使用计算图和自动微分，我们可以轻松地计算损失函数相对于模型参数的梯度，并使用梯度下降等优化算法更新模型参数。

下面是一个简单的示例，演示了如何构建计算图、进行正向计算和反向传播：

~~~
import torch

# 创建张量
x = torch.tensor(3.0, requires_grad=True)
y = torch.tensor(2.0, requires_grad=True)

# 执行计算操作
z = x**2 + y**3

# 进行反向传播
z.backward()

# 打印梯度
print(x.grad)  # 输出: 6.0
print(y.grad)  # 输出: 12.0

~~~

在上述示例中，我们首先创建了两个张量x和y，并将它们的`requires_grad`属性设置为True，以指示PyTorch需要计算它们的梯度。

然后，我们通过对x进行平方和对y进行立方的操作，得到了一个新的张量z。在这个过程中，PyTorch会自动构建计算图，记录了张量之间的依赖关系。

最后，我们调用了z的`.backward()`方法，触发自动微分过程。PyTorch根据计算图自动计算了z相对于x和y的梯度，并将结果存储在相应的grad属性中。

通过使用计算图和自动微分，我们可以轻松地构建复杂的神经网络模型，并使用反向传播算法进行训练和优化。

该代码定义了以下计算图

![image-20230515231623340](../../../AppData/Roaming/Typora/typora-user-images/image-20230515231623340.png)

在这个网络中，w和b是需要优化的参数。因此，我们需要能够计算损失函数相对于这些变量的梯度。为了实现这一点，我们需要将这些张量的requires_grad属性设置为True。

确实，可以在创建张量时设置requires_grad属性的值，也可以后续使用`x.requires_grad_(True)`方法来更改requires_grad属性的值

PyTorch中的函数（Function）实际上是用于构建计算图的对象。这个对象知道如何在前向传播中计算函数的结果，并且在反向传播过程中计算其导数。张量的grad_fn属性中存储了对应的反向传播函数的引用。Function类是PyTorch中的基础类，它定义了计算图中每个操作的前向计算和反向传播的逻辑。每个张量都有一个与之关联的grad_fn属性，用于追踪它在计算图中的来源操作和梯度计算函数。你可以通过访问张量的grad_fn属性来获取与该张量相关的反向传播函数的信息。这个属性可以帮助你理解计算图的结构，并了解每个操作是如何影响梯度的计算和传播的。你可以通过访问张量的grad_fn属性来获取与该张量相关的反向传播函数的信息。这个属性可以帮助你理解计算图的结构，并了解每个操作是如何影响梯度的计算和传播的。

~~~
print(f"Gradient function for z = {z.grad_fn}")
print(f"Gradient function for loss = {loss.grad_fn}")
~~~

输出显示了与z和loss张量相关的反向传播函数的类型和地址。

这些反向传播函数存储了用于计算梯度的逻辑，并在反向传播过程中自动计算并传播梯度。了解这些反向传播函数的存在和功能可以帮助我们理解梯度计算和反向传播的原理。

## Computing Gradients

要**计算神经网络中参数的梯度**，我们需要计算损失函数相对于参数的导数，即∂loss/∂w和∂loss/∂b，在某些固定的x和y值下。为了计算这些导数，我们调用loss.backward()，然后从w.grad和b.grad中获取相应的值。

~~~
import torch

x = torch.ones(5)
y = torch.zeros(3)
w = torch.randn(5, 3, requires_grad=True)
b = torch.randn(3, requires_grad=True)
z = torch.matmul(x, w) + b
loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)

loss.backward()
print(w.grad)
print(b.grad)
~~~

在上述代码中，我们首先计算了z和loss的值。然后，我们调用loss.backward()来计算梯度。最后，我们分别打印了w.grad和b.grad的值。

输出显示了w.grad和b.grad的值，这些值是根据loss函数对w和b的梯度计算得到的。

这样，我们就可以利用这些梯度值来进行参数的优化，例如使用梯度下降算法更新参数。通过调用backward()方法，PyTorch会自动计算梯度，并将其存储在对应的grad属性中，以便我们可以使用它们进行后续的优化步骤。

Note:

在计算图中，我们只能获取具有requires_grad属性设置为True的叶节点的梯度（grad属性）。对于计算图中的其他节点，梯度将不可用。

出于性能原因，我们在给定的计算图上只能进行一次反向传播的梯度计算。如果我们需要在同一个计算图上进行多次反向传播计算，我们需要在backward调用中传递retain_graph=True参数。

~~~
import torch

x = torch.tensor(2.0, requires_grad=True)
y = x ** 2
z = y ** 2

# Perform backward pass on the graph
z.backward(retain_graph=True)

print(x.grad)  # Prints the gradient of x

# Perform backward pass again on the same graph
y.backward()  # Passing retain_graph=True is not required this time

print(x.grad)  # Prints the updated gradient of x

~~~

第一个打印语句打印了初始的x的梯度（64），而第二个打印语句打印了更新后的x的梯度（128）。

请注意，当我们进行多次反向传播计算时，只有第一次需要传递retain_graph=True参数。在后续的反向传播计算中，我们不需要再传递该参数，PyTorch会自动管理计算图的释放和重建。

## Disabling Gradient Tracking

**禁用梯度跟踪**

默认情况下，所有具有requires_grad=True属性的张量都会跟踪其计算历史并支持梯度计算。然而，在某些情况下，我们不需要这样做，例如当我们已经训练好模型，只想将其应用于一些输入数据时，即只需要通过网络进行前向计算。我们可以使用torch.no_grad()代码块来停止跟踪计算过程：

~~~
z = torch.matmul(x, w)+b
print(z.requires_grad)

with torch.no_grad():
    z = torch.matmul(x, w)+b
print(z.requires_grad)
~~~

在上述代码中，我们首先打印了z的requires_grad属性，该属性为True，表示z会跟踪计算历史并支持梯度计算。然后，我们使用torch.no_grad()代码块包裹了一段计算代码，在该代码块中的计算将不会被跟踪。在计算完成后，我们再次打印了z的requires_grad属性，该属性为False，表示z不再跟踪计算历史并且不支持梯度计算。

请注意，使用torch.no_grad()代码块只会暂时停止梯度跟踪，只在包裹的代码块中有效。在代码块之外，梯度跟踪仍然有效。这在推理阶段非常有用，可以提高计算效率并减少内存消耗。

另一种实现相同结果的方法是使用张量的detach()方法：

~~~
z = torch.matmul(x, w)+b
z_det = z.detach()
print(z_det.requires_grad)
~~~

在上述代码中，我们首先计算了z，然后使用detach()方法创建了一个新的张量z_det，该张量不再与计算图相关联。我们打印了z_det的requires_grad属性，结果为False，表示z_det不会跟踪计算历史并且不支持梯度计算。

使用detach()方法可以将一个张量从计算图中分离出来，使其成为一个独立的张量，不再与原始计算相关联。这在需要保留某个计算结果的值，但不需要进行梯度计算时非常有用。

Note：

有几个原因可能会让您禁用梯度跟踪：

1. 标记神经网络中的某些参数为冻结参数：在神经网络中，有时候我们希望固定一些参数，不对其进行梯度更新。这可以通过将这些参数的requires_grad属性设置为False来实现，从而禁用它们的梯度跟踪。这样可以确保在反向传播过程中不会对这些参数进行梯度计算和更新。
2. 在只进行前向传播时加快计算速度：当您只需要进行前向传播，而不需要进行梯度计算和反向传播时，禁用梯度跟踪可以提高计算效率。不跟踪梯度的张量计算起来更高效，因为不需要存储梯度相关的中间结果，节省了内存和计算资源。

禁用梯度跟踪可以帮助优化计算性能，并使代码更加清晰和高效。

## More on Computational Graphs

**更多的关于计算图**

在概念上，autograd在一个有向无环图（DAG）中记录数据（张量）和执行的所有操作（以及生成的新张量）。在这个DAG中，叶节点是输入张量，根节点是输出张量。通过从根节点到叶节点的追踪，可以使用链式法则自动计算梯度。

在前向传播过程中，autograd同时执行两个操作：

1. 运行请求的操作以计算结果张量。
2. 在DAG中维护操作的梯度函数。

当在DAG的根节点上调用.backward()时，反向传播开始执行。autograd会：

1. 从每个.grad_fn计算梯度。
2. 将梯度累积到相应张量的.grad属性中。
3. 使用链式法则将梯度传播到叶节点张量。

注意：

在PyTorch中，DAG是动态的。重要的一点是，每次调用.backward()时，都会从头开始重新创建图。这正是允许您在模型中使用控制流语句的原因；如果需要，您可以在每次迭代中更改形状、大小和操作。

这种动态性使得PyTorch非常灵活，可以根据需要动态调整模型的结构和计算流程。

## Optional Reading: Tensor Gradients and Jacobian Products

**可选阅读: 张量梯度和雅可比积**

在许多情况下，当输出函数是任意张量时，我们需要计算相对于参数的梯度。PyTorch提供了一种计算雅可比乘积（Jacobian Product）的方法，而不是实际的梯度。

![image-20230515235307720](../../../AppData/Roaming/Typora/typora-user-images/image-20230515235307720.png)

PyTorch允许您通过调用 backward() 函数并传入 v 作为参数来计算雅可比乘积 v ⋅ J 。这在处理具有张量输出的函数时非常有用，而且当您只需要与给定向量的雅可比乘积时十分方便。

~~~
inp = torch.eye(4, 5, requires_grad=True)
out = (inp+1).pow(2).t()
out.backward(torch.ones_like(out), retain_graph=True)
print(f"First call\n{inp.grad}")
out.backward(torch.ones_like(out), retain_graph=True)
print(f"\nSecond call\n{inp.grad}")
inp.grad.zero_()
out.backward(torch.ones_like(out), retain_graph=True)
print(f"\nCall after zeroing gradients\n{inp.grad}")
~~~

在这个示例中，我们有一个输入张量 `inp`，然后计算输出张量 `out`。我们使用 `torch.ones_like(out)` 作为参数调用 backward() 来计算雅可比乘积。梯度会累积在 `inp.grad` 张量中。

请注意，当我们第二次调用带有相同参数的 backward 函数时，梯度的值是不同的。这是因为在反向传播过程中，PyTorch会累积梯度，即计算得到的梯度值会添加到计算图中所有叶子节点的 grad 属性中。如果您想正确计算梯度，需要在计算之前将 grad 属性清零。在实际训练中，优化器会帮助我们完成这个操作。

注意：

之前我们调用 backward() 函数时没有传入参数。实际上，这等价于调用 backward(torch.tensor(1.0))，这是在计算标量值函数（如神经网络训练中的损失函数）的梯度时的一种有用方式。

# 06.OPTIMIZING MODEL PARAMETERS

**优化模型参数**

现在我们有了模型和数据，是时候通过优化参数来训练、验证和测试我们的模型了。训练模型是一个迭代的过程；在每次迭代中，模型对输出进行猜测，计算其猜测的错误（损失），收集关于参数的错误导数（正如我们在前面的部分中所看到的），并使用梯度下降来优化这些参数。如果想详细了解这个过程，请观看3Blue1Brown([反向传播演算|附录深入学习第3章 - YouTube](https://www.youtube.com/watch?v=tIeHLnjs5U8))的这个关于反向传播的视频。

## Prerequisite Code

**先决条件代码**

 我们加载了前面章节中的数据集和数据加载器以及构建模型的代码。

~~~
import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision.transforms import ToTensor

training_data = datasets.FashionMNIST(
    root="data",
    train=True,
    download=True,
    transform=ToTensor()
)

test_data = datasets.FashionMNIST(
    root="data",
    train=False,
    download=True,
    transform=ToTensor()
)

train_dataloader = DataLoader(training_data, batch_size=64)
test_dataloader = DataLoader(test_data, batch_size=64)

class NeuralNetwork(nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, 10),
        )

    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits

model = NeuralNetwork()
~~~

## Hyperparameters

**超参数**

是可以调整的参数，让你控制模型优化过程。不同的超参数值可能会影响模型训练和收敛速度（关于超参数调整请阅读更多）

我们为训练定义以下超参数：

 Epochs-迭代数据集的次数

Batch Size-在更新参数之前通过网络传播的数据样本数量

Learning Rate-每个 batch/epoch 更新模型参数的程度。较小的值会产生较慢的学习速度，而较大的值可能会导致训练过程中不可预测的行为。

learning_rate = 1e-3

 batch_size = 64

 epochs = 5

## Optimization Loop

**优化循环**（Optimization Loop） 在设置了超参数之后，我们可以使用优化循环来训练和优化模型。

优化循环的每个迭代称为一个 epoch。

每个 epoch 包含两个主要部分： 

训练循环（Train Loop）- 遍历训练数据集并尝试收敛到最佳参数。

验证/测试循环（Validation/Test Loop）- 遍历测试数据集以检查模型性能是否在改善。



让我们简要了解一下训练循环中使用的一些概念。请继续阅读以查看优化循环的完整实现。

### Loss Function

**损失函数**（Loss Function） 当给定一些训练数据时，我们的未经训练的网络很可能无法给出正确的答案。损失函数用于衡量获取的结果与目标值之间的不相似程度，而在训练过程中我们希望最小化这个损失函数。为了计算损失，我们使用给定数据样本的输入进行预测，并将其与真实数据标签值进行比较。

常见的损失函数包括用于回归任务的 nn.MSELoss（均方误差）和用于分类任务的 nn.NLLLoss（负对数似然）。nn.CrossEntropyLoss 结合了 nn.LogSoftmax 和 nn.NLLLoss。

我们将模型的输出 logits 传递给 nn.CrossEntropyLoss，它将对 logits 进行归一化并计算预测误差。

~~~
# Initialize the loss function
loss_fn = nn.CrossEntropyLoss()
~~~

### Optimizer

**优化器**（Optimizer） 优化是调整模型参数以减少每个训练步骤中的模型误差的过程。

优化算法定义了这个过程的执行方式（在本例中我们使用随机梯度下降）。所有的优化逻辑都封装在优化器对象中。在这里，我们使用 SGD 优化器；此外，PyTorch 中还有许多不同的优化器可用，如 ADAM 和 RMSProp，它们适用于不同类型的模型和数据。

我们通过注册需要训练的模型参数并传入学习率超参数来初始化优化器。

~~~
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
~~~

在训练循环内部，优化分为三个步骤： 

调用 `optimizer.zero_grad()` 来重置模型参数的梯度。默认情况下，梯度是累加的；为了防止重复计算，我们在每次迭代时明确将其归零。

通过调用 `loss.backward()` 反向传播预测损失。PyTorch 会计算每个参数相对于损失的梯度。

一旦我们有了梯度，我们调用 `optimizer.step()` 来根据反向传播中收集到的梯度来调整参数。

## Full Implementation

**完整实现**

以下是完整的优化循环实现，包括训练和测试循环：

~~~
def train_loop(dataloader, model, loss_fn, optimizer):
    size = len(dataloader.dataset)
    for batch, (X, y) in enumerate(dataloader):
        # Compute prediction and loss
        pred = model(X)
        loss = loss_fn(pred, y)

        # Backpropagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if batch % 100 == 0:
            loss, current = loss.item(), (batch + 1) * len(X)
            print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")


def test_loop(dataloader, model, loss_fn):
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    test_loss, correct = 0, 0

    with torch.no_grad():
        for X, y in dataloader:
            pred = model(X)
            test_loss += loss_fn(pred, y).item()
            correct += (pred.argmax(1) == y).type(torch.float).sum().item()

    test_loss /= num_batches
    correct /= size
    print(f"Test Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")
~~~


这段代码实现了训练循环和测试循环，用于优化模型和评估模型性能。

`train_loop`函数接受训练数据集的`dataloader`、模型`model`、损失函数`loss_fn`和优化器`optimizer`作为输入。它遍历训练数据集的批次，并执行以下步骤：

- 计算模型对输入数据`X`的预测值`pred`。
- 计算预测值`pred`和实际标签`y`之间的损失`loss`。
- 清零优化器的梯度缓存，以防止梯度累积。
- 通过调用`loss.backward()`执行反向传播，计算参数的梯度。
- 调用`optimizer.step()`更新模型的参数，根据梯度进行优化。
- 如果批次数是100的倍数，打印当前批次的损失值和处理的样本数。

`test_loop`函数接受测试数据集的`dataloader`、模型`model`和损失函数`loss_fn`作为输入。它用于评估模型在测试数据集上的性能，包括计算平均损失和准确率。

- 遍历测试数据集的批次，对每个批次执行以下操作：
	- 使用模型对输入数据`X`进行预测。
	- 计算预测值`pred`和实际标签`y`之间的损失，并将其累加到`test_loss`中。
	- 统计预测正确的样本数量，并将其累加到`correct`中。
- 计算平均损失`test_loss`和准确率`correct`。
- 打印测试错误信息，包括准确率和平均损失。

这些循环一起组成了模型的训练和测试过程，通过迭代优化参数和评估模型性能来训练和调整模型。

我们初始化损失函数和优化器，并将它们传递给`train_loop`和`test_loop`函数。您可以随意增加训练轮次的数量，以跟踪模型性能的改进。

~~~
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

epochs = 10
for t in range(epochs):
    print(f"Epoch {t+1}\n-------------------------------")
    train_loop(train_dataloader, model, loss_fn, optimizer)
    test_loop(test_dataloader, model, loss_fn)
print("Done!")
~~~

这段代码实现了一个完整的训练过程，包括多个训练轮次（epochs）。下面是对代码的解释：

首先，我们定义了损失函数 `loss_fn` 为交叉熵损失函数 (`nn.CrossEntropyLoss()`)，用于计算模型的预测结果与真实标签之间的损失。

然后，我们定义了优化器 `optimizer`，使用随机梯度下降算法 (`torch.optim.SGD`)，并将模型的参数和学习率传递给优化器。

接下来，我们通过一个循环来进行多个训练轮次。每个训练轮次都以 `Epoch {t+1}` 的形式进行打印，其中 `t` 表示当前轮次的索引。

在每个训练轮次中，我们调用 `train_loop` 函数来遍历训练数据集并进行模型优化。在 `train_loop` 函数中，我们计算模型的预测结果和损失，然后进行反向传播和参数更新。

接着，我们调用 `test_loop` 函数来评估模型在测试数据集上的性能。在 `test_loop` 函数中，我们计算测试数据集上的损失和准确率。

最后，当所有训练轮次完成后，打印出 "Done!" 表示训练过程结束。

您可以根据需要调整训练轮次的数量 (`epochs`)，以便更好地追踪模型性能的改进。

# 07.SAVE & LOAD MODEL

在本节中，我们将研究如何通过保存、加载和运行模型预测来保持模型状态。

~~~
import torch
import torchvision.models as models
~~~

## Saving and Loading Model Weights

PyTorch模型将学习到的参数存储在一个名为`state_dict`的内部状态字典中。可以使用`torch.save`方法将其保存到文件中。

~~~
model = models.vgg16(weights='IMAGENET1K_V1')
torch.save(model.state_dict(), 'model_weights.pth')
~~~

这段代码展示了如何使用PyTorch的`torchvision.models`模块加载预训练的VGG16模型，并将其权重（参数）保存到名为`model_weights.pth`的文件中。

1. 首先，通过`models.vgg16(weights='IMAGENET1K_V1')`创建了一个VGG16模型的实例。`weights='IMAGENET1K_V1'`表示使用在ImageNet数据集上训练的预训练权重。
2. 接下来，使用`model.state_dict()`获取VGG16模型的当前参数状态字典。`state_dict()`方法返回一个Python字典对象，其中包含模型的所有参数及其对应的张量值。
3. 最后，使用`torch.save()`函数将参数状态字典保存到名为`model_weights.pth`的文件中。这个文件可以在以后的训练或预测任务中用来加载模型的权重。

`vgg16` 是一个预训练的卷积神经网络模型，它属于 torchvision.models 模块提供的模型之一。VGG16 是由牛津大学的研究团队提出的一个深度卷积神经网络模型，它在 ImageNet 数据集上进行了训练，并在图像分类任务中取得了良好的性能。

VGG16 模型具有 16 层深的网络结构，其中包含卷积层、池化层和全连接层。它的主要特点是使用了小尺寸的 3x3 卷积核，连续堆叠多个卷积层来构建深层网络。由于它的简单有效性和可扩展性，VGG16 成为了许多计算机视觉任务中的基准模型之一。

在给定的代码中，通过使用 `models.vgg16()` 函数创建了一个 VGG16 模型的实例，并传递了参数 `weights='IMAGENET1K_V1'`，表示加载预训练的权重。这样，创建的模型就具备了在 ImageNet 数据集上进行训练得到的权重参数。接着，使用 `torch.save()` 函数将该模型的参数（即状态字典）保存到名为 `model_weights.pth` 的文件中，以便以后在其他任务中加载和使用这些参数。

要加载模型权重，首先需要创建同一模型的实例，然后使用 `load_state_dict()` 方法加载参数。

~~~
model = models.vgg16() # we do not specify ``weights``, i.e. create untrained model
model.load_state_dict(torch.load('model_weights.pth'))
model.eval()
~~~

在这段代码中，首先我们创建了一个未经训练的VGG16模型实例 `model = models.vgg16()`，在这里我们没有指定 `weights` 参数，因此模型是未经训练的。接下来，我们使用 `load_state_dict()` 方法加载之前保存的模型权重文件 `'model_weights.pth'`。通过这一步，模型的参数被恢复为保存的状态。最后，我们调用 `model.eval()` 将模型设置为评估模式，这将影响模型在推断阶段的行为，例如启用了批标准化的评估模式和Dropout的关闭。

总结起来，这段代码的作用是加载之前保存的模型权重，并将模型设置为评估模式，以便进行推断或测试。

确保在进行推断之前调用 `model.eval()` 方法，将 dropout 和批量归一化层设置为评估模式。如果不这样做，将会导致推断结果不一致。

## Saving and Loading Models with Shapes

在加载模型权重时，我们需要首先实例化模型类，因为该类定义了网络的结构。如果我们想要将该类的结构与模型一起保存，那么可以将模型（而不是model.state_dict()）传递给保存函数。

~~~
torch.save(model, 'model.pth')
~~~

We can then load the model like this:

~~~
model = torch.load('model.pth')
~~~

这种方法在序列化模型时使用了Python的pickle模块，因此在加载模型时需要确保实际的类定义可用。





**That’s all. Thanks for reading.**